---
layout:     post
title:      "Machine Learning "
subtitle:   "Key Concept"
date:       2020-05-13 20:00:00
author:     "Becks"
header-img: "img/post-bg-os-metro.jpg"
catalog:    true
tags:
    - Machine Learning
    - 学习笔记
---

<style type="text/css">
.image-left {
  display: block;
  margin-left: auto;
  margin-right: auto;
  float: left;
}
</style>


## Logit Function 

$$Odd = \frac{P }{1- P}, text{ where p is probability}$$

$$\color{fuchsia}{\bf{odd} = \frac{\text{Something happening i.e. wining}}{ \text{Something not happening. i.e. losing }}}$$


$$\color{blue}{\bf{probability} = \frac{\text{Something happening i.e. wining}}{ \text{everything could happen. i.e. losing + winning}}}$$


P is the probability and odds is not probability. E.g. Winning a game probability is 0.7, Odds of winning is: $Odd = \frac{7}{3}$


**Log of odds**: the asymmetry of odds make it diffcult to compare the odds for something happening or not happening.

$$ - log \left( \frac{p}{1-p}   \right) =  log \left( \frac{1-p}{p}   \right)$$

E.g. If the probability of winning is 0.9, the odds of winning is 9, the odds of losing $\frac{1}{9}$

![](/img/post/ML/logit1.png)

**Derived Sigmoid Function**

$$
\begin{aligned}
log \left( \frac{p}{1-p}   \right) &= W X + b \\
\frac{p}{1-p} &= e^{WX + b} \\
p &= e^{WX + b} - p e^{WX + b} \\
P &= \frac{e^{WX + b}}{ 1 + e^{WX + b}} \\
   &=  \frac{1}{ 1 + e^{-\left(WX + b\right)}} \\
\end{aligned}
$$

![](/img/post/ML/logit2.png)



## Decision Tree

- **Advantage**: easy to use / train and easy to interpret. 
- <span style="background-color:#FFFF00">**Disdvantage**: not flexible when comes to classifying new samples, often being overfit</span>


**Entropy**: a measure of disorder or uncertainty. High Value means high disorder. 

$$E\left(S \right) = -\sum_{i=1}^c P\left( x_i \right) log_2 P\left( x_i \right)$$


![](/img/post/ML/entropy.png)

**Information Gain**:  the higher the value, the more gains




- **Decision Trees** algorithm will always tries to maximize **Information gain**.
- An **attribute** with highest Information gain will tested/split first.

$$
\begin{aligned}
IG \left(Y,X \right) &= E\left(  Y \right) - E \left( Y \mid X \right) \\
& = E \left( Y \right) -\big( p\left( x_1 \right) E \left( Y \mid X = x_1 \right) + p\left( x_2 \right)E\left( Y \mid X= x_2 \right) \big)

\end{aligned}
$$


e.g.  Use if height is bigger than 190 cm to predict if a person is a NBA player

| Height >  190cm (X) | Is NBA player (Y) |
| :-----: | :-----: |
|  Yes   |  Yes     |
|  No    |  Yes     |
|  Yes   |  No     |
|  No   |  No     |
|  Yes   |  Yes     |

$$E(Y) =  - \frac{2}{5} * log_2 \left( \frac{2}{5}  \right) -  \frac{3}{5} * log_2 \left( \frac{3}{5}  \right) = 0.971 $$

$$E(Y | X = Yes) =  - \frac{1}{3} * log_2 \left( \frac{1}{3}  \right) -  \frac{2}{3} * log_2 \left( \frac{2}{3}  \right) = 0.918 $$

$$E(Y | X = NO) =  - \frac{1}{2} * log_2 \left( \frac{1}{2}  \right) -  \frac{1}{2} * log_2 \left( \frac{1}{2}  \right) = 1 $$

$$
\begin{aligned}
IG \left(Y,X \right) &= E\left(  Y \right) -  \big( p\left( X=YES \right) E \left( Y \mid X = YES \right) + p\left( X = No \right)E\left( Y \mid X= NO \right) \big) \\

&= 0.971 - \left( \frac{3}{5} *0.918 +  \frac{2}{5}*1 \right) \\
&= 0.02
\end{aligned}
$$


For **Numerical Values**, Sort the value, calculate the <span style="background-color:#FFFF00">**average for all adjacent points**</span>, then use each average value as the threshold to compute information gain and find the best one

![](/img/post/ML/dt1.png)

When dealing with **rank values**. Try <span style="color:red">each rank as threshold and find the best one.</span> 
e.g. only has 1 to 4 rank.  Note: 不用试 rank = 4 as threshold 因为所有rank 都小于4

![](/img/post/ML/dt2.png)

When dealing with **multiple categorical values**: Try each possible combination and find the best one 

![](/img/post/ML/dt3.png)

## Regression Tree

The failure of Linear Regression:

![](/img/post/ML/regressionTree1.png)

- Regression Tree are a type of decision tree. 
  - <span style="background-color:#FFFF00">In regression tree, each leaf represents a **numeric value**</span>
  - Classification Trees have True or False or some discrete category in their leaves
- Regression Tree easily accomodates additional predictors 
- To find the **threshold** for each node: <span style="background-color:#FFFF00">split the data into two groups by finding the threshold that **gave the smallest sum of squared residuals**</span> $$\sum \left( predicted - observed \right)^2$$
- When there are <span style="background-color:#FFFF00">**multiple features**, try different features with different threshold. Then use the feature with the threshold that has **the smallest sum of squared residuals** as the node and threshold</span>
- <span style="color:red">The leaf values is the **average of observation values** ending within this leaf</span>
- If each leaf end with only one observation, the accuracy of training is 100% but it will overfit the training data. <span style="background-color:#FFFF00">**To aviod overfitting**</span>:
  1. <span style="color:red">Only split observations when there are more than some minimum number</span>. Typically the minimum number of observations to allow for a split is 20
  2. Remove some of leaves and combine them into one leaf. The predicted value of the leaf is the average of all values


E.g. Use employee's Income to predict car value

![](/img/post/ML/regressionTree2.png)


![](/img/post/ML/regressionTree3.png)


#### Cost Complexity Prune Tree

- How to prune the tree to avoid the overfitting on training data (not perform so well on training data )
- **Tree Score**: based on **Sum of Squared Residuals** (SSR) for the tree or sub-tree and **Tree Complexity Penalty(T)** that is a function of the number of leaves, or Terminal nodes in the tree or sub-tree
  - **Tree Complexity Penalty** compensates for the difference in the number of leaves
  - $\alpha$ is a tuning parameter. Find the best $\alpha$ using **cross validation**

$$\bf{\text{Tree Score } = SSR + \alpha \underbrace{T}_{\# Leaves}}$$

e.g. tree has four leaves and $\alpha = 10000$

$$\bf{\text{Tree Score }} = 543.8 + 10000 \times 4 $$


**Step 1: <span style="background-color:#FFFF00">Tunning $\alpha $</span> using use all of the data (not just training)**
  1. build the full-sized regression Tree (which do the best on training data). It has the lowest **Sum of Squared Residuals** among all the trees(**$\alpha = 0$**)
  2. Increase $\color{red}{\bf{\alpha}}$ until pruning leaves(reducing number of leaves) give us a ower Tree score 
  3. Repeat Step 2. then different values for $\color{red}{\bf{\alpha}}$ give a sequence of trees from full sized to just a leaf. (每个tree 对应一个 $\alpha$)

 ![](/img/post/ML/regressionTree4.png)

**Step 2: <span style="background-color:#FFFF00">Selecting the best $$\alpha$$. Divide data into training and testing. </span>**
  1. Use the $$\alpha$$ found at the step 1 to build a full tree and a sequence of subtrees that minimize the Tree Score for each sub-tree
  2. Repeat above step K times (**K-fold cross validation**), then pick the $$\alpha$$ which has <span style="background-color:#FFFF00">lowest average **Sum of Square Residuals** with **Testing Data**</span> (如果有N 个tree, 有N个$$\alpha$$对应，选择最小的average **Sum of Square Residuals**的 $$\alpha$$)



**Step 3: pick the tree build at Step 1(with full data) that corresponds to the value for $$\alpha$$ selected at Step 2  as the final pruned tree**





## Random Forest 

Random Forest combine the simplicity of decision tress with flexibility resulting in a vast improvement in accuracy. 


#### Build Random Forest

**Step 1: Bootstrap** Create new dataset, <span style="background-color:#FFFF00">**same size as original**</span>, randomly select samples from original dataset and allows <span style="background-color:#FFFF00">**duplicate**</span>

- **Bagging**:<span style="background-color:#FFFF00">combine classifiers by sampling/bootstrapping training set</span>
- <span style="background-color:#FFFF00">**Out-of-bag dataset**</span>: Around 1/3 of original data doesnot end up in the bootstrap dataset

![](/img/post/ML/rt1.png)

**Step 2**: Create a decision tree using the bootstrapped dataset but only use a **random subset of variables** at each step 
  - e.g. total features = 4 at first node, only **randomly select 2 out of 4** features to see which one of 2 is best for the first node. Then to select second nodes, **randomly select 2 out of 3** features to see wich is the best

**Step 3**: Repeat step 1 and 2 N times, and generate a wide variety of trees. 
  - The variety makes random forests more effective than individual decision trees.

#### Evaluation 

run all out-of-bag samples for all of trees and see if the predictions are correct or not.

- For each evaluation sample, <span style="color:red">run through all N trees and use the most predicted class as output</span>
  - e.g. total 100 trees, 80 of them predict a person is a NBA player and 20 of them predict not NBA player. Output = NBA player

$$Accuracy = \frac{\text{correct labeled prediction for out-of-bag dataset}}{\text{total out-of-bag size}}$$

$$\bf{\text{Error of out-of-bag samples}} = \frac{\bf{\text{incorrect}} \text{  classification for out-of-bag dataset}}{\text{total out-of-bag size}}$$

e.g. total 100 out-of-bag size, 80 of them predict correct(<span style="color:red">running all trees, label is same as most predict classes</span>), Accuracy = 0.8

#### Missing Values

![](/img/post/ML/rt2.png)


**Deal with missing data in original dataset**

**Step 1: Make a initial guess**: 

- <span style="background-color:#FFFF00">**For categorical value**</span>:  guess the <span style="background-color:#FFFF00">**common class**</span> among other samples that have the same label.  
- <span style="background-color:#FFFF00">**For numeric value**</span>:  guess the <span style="background-color:#FFFF00">**median value**</span> among other samples that have the same label. 
  - e.g. In above picture, label of missing row  *No Heart Disease*, then find for other No Heart Disease sample, the median of weight is (125 + 210)/2 = 167.5

**Step 2: Build the random forest using all samples including guessed ones**: 


**Step 3: Run all of data for all the tree and build <span style="background-color:#FFFF00">Proximity Matrix</span>**


$$\text{Proximity value} = \frac{\color{fuchsia}{\text{the number of times two data end with the same leaf node}}}{\text{total random forest trees}}$$

$$Distance = 1-  \text{Proximity value}$$

<span style="background-color:#FFFF00">If Distance close to 0, means two data end with teh same leaf node for all trees </span>

![](/img/post/ML/rt3.png)
![](/img/post/ML/rt4.png)


**Step 4: use the proximity values to make better guess about the missing data**

![](/img/post/ML/rt5.png)

Because  $$\frac{3}{5} > \frac{1}{30}$$, esitmated value is no 

$$\begin{aligned} \text{Weight Guess} &= 125*\frac{0.1}{0.1+0.1+0.8}  \\
& +180*\frac{0.1}{0.1+0.1+0.8} \\ & + 210*\frac{0.8}{0.1+0.1+0.8} = 198.5 \end{aligned}$$


**Step 5: Repeat step 1 and 3 over and over again to build the tree, run the data through the tree, and recalculate the missing value <span style="background-color:#FFFF00">until missing value converge</span>. No longer change each time we recalculate**

***

**Deal with missing data in new sample that need to be predicted**


**Step 1**: Create <span style="background-color:#FFFF00">**n (total n classes) copies of the data**</span>, one for each predicted class`
  - e.g. data is missing for height > 190cm. Make 2 copies of data, and label the first is NBA player and another second not a NBA player

**Step 2**: use above iterative methods to <span style="color:red">make a good prediction about the missing values for each copy of labeled class</span>
  - e.g. guess height > 190cm for both copies 

**Step 3**: Then run those n copies of data through the trees in the forest with filled estimation value, and see <span style="background-color:#FFFF00">**which one is correctly labeled by the random forest the most times**</span>
  - e.g. total 100 trees, Predict 80 as NBA player for the one labeled as NBA player.  Predict 60 as not NBA player for the one labeled as not a NBA player. Finally output as NBA player


## AdaBoost

- Random Forest, make a full size tree and some tree may bigger than others
- In a Forest of Trees made with **AdaBoost**, the trees are usually just <span style="background-color:#FFFF00">**a node and two leaves(called Stump)**</span>
  - Stump are not great at making accurate classifications
- AdaBoost combines a lot of **weak learners**(**stumps**) to make clasfication
- <span style="color:red">Some Stumps get more say</span> in the classification than others
- Each stump is made by <span style="color:red">**taking previous stump's mistakes into account**</span>
- In	practice,	we	usually	<span style="background-color:#FFFF00">**stop boosting	after	certain	iterations	to	both save	time	and	prevent	overfitting**</span>


![](/img/post/ML/ab1.png)

In **Random Forest**, each tree has an **equal vote** on the final classification. However for AdaBoost in a **Forest of Stumps**, some stumps get more say in final classification than others

![](/img/post/ML/ab2.png)

In **Random Forests**, each decision tree is made <span style="background-color:#FFFF00">**independently**</span> of the others. In contrast, <span style="background-color:#FFFF00">**in a Forest of Stumps made with AdaBoost, order is important**</span>
  - The error of first stump made influence how second stump made. Then error of second stump influence the third stump.


#### Create a Forest of Stumps using AdaBoost

**Step 1:** <span style="background-color:#FFFF00">**Give each sample a weight**</span> that indicates how important it is to be correctly classified. At start, all sample get a same weight. 

$$weight = \frac{1}{\text{total number of samples }}$$

**Step 2: Then choose the best feature as the node of stump**


**Step 3: Calculate Total Error and Amount Say** for the stump. Total Error is always between 0 and 1. <span style="color:red">0 is perfect stump and 1 for horrible stump</span>
  - when stump does a good job, **Total Error** is small and **Amount of Say** is large
  - When stump like flipping a coin, **Total Error** = 0.5 and **Amount of Say** is 0
  - When stump consistently gives the opposite classification, **Total Error** is close to 1 and **Amount of Say** is a large negative value

$$\text{Total Error} = \frac{\text{total data being incorrectly classified}}{\text{total number of dataset}} $$

$$\text{Amount of Say } \left( \alpha_t \right) = \frac{1}{2} log\left( \frac{1 - \text{Total Error}}{\text{Total Error }} \right)$$

**Step 4: Use the amount of say to Adjust the weight**. The idea is to increase weight for incorrectly classified sample and decrease weight for correctly classified sample

$$\text{Incorrectly classified Sample Weight} = \text{sample weight} \times \color{fuchsia}{e^{\text{amount of say}}}$$

$$\text{correctly classified Sample Weight} = \text{sample weight} \times \color{fuchsia}{e^{-\text{amount of say}}}$$


![](/img/post/ML/ab3.png)

- For **incorrectly classfied** sample: 
  - If **amount of say** is large (<span style="color:red">stump does a good job</span>), <span style="color:red">new Sample weight will be much larger than old one</span>
  - If  **amount of say** is small (<span style="color:red">stump does not do a good job</span>), <span style="color:red">new Sample weight will be a little larger than old one</span>
- For **correctly classfied** sample: 
  - If **amount of say** is large, new sample weight will very small
  - If **amount of say** is small, new sample weight will very a little smaller than the old one

**Step 5: Normalize new weights to make them add to 1**

Combine step 4 and step 5:

$$
\begin{aligned}
  
W_{t+1}\left( i \right) & = \frac{W_{t}\left( i \right)}{Z_t} \times 

             \begin{cases} e^{-\alpha_t} & \text{if } h_t\left( x_i \right) = y_i \\
                           e^{\alpha_t} & \text{if } h_t\left( x_i \right) \neq y_i

              \end{cases} \\
& =  \frac{W_{t}\left( i \right)  exp\left( -\alpha_t y_i h_t\left(x_i\right) \right)  }{Z_t}
\end{aligned}
$$


where $$h_t\left( x_i \right)$$ is the prediction of stump, both  $$h_t\left( x_i \right)$$ and $$y_i$$ only take 1 or -1
 

**Step 6: Gnerate sample based on the new noralized weight(<span style="background-color:#FFFF00">allow duplicate and the same size as before</span>)**: incorrect classified data will be drawn with a higher probability since the weight is higher

**Step 7: Use new genereated sample as dataset and change the weight to equal weight**
- Note: equal weight doesn't mean stump will not emphasize the need to correctly classify these samples. Because allow duplicate, in new generated sample there is more data of being incorrectly classfied before

$$weight = \frac{1}{\text{total number of samples }}$$

<span style="color:red">Or Substitution of Step 6 and 7: use new weight in Information gain/Weighted Gini Function mismatches without generating new dataset</span>

**Step 8: Repeat step 1-7 N times to make N stumps**
- So error of first tree influence second tree, then second tree error influence third tree

**Evaluation**:

Use the data to pass all the stump and calculate the **total amount of say(created during the training)** from each predicted class, <span style="background-color:#FFFF00">the class with total largest of amount of say is the prediction class</span>

$$\hat y = sign \left( \sum_{t=1}^T \alpha_t h_t \left(x \right) \right), \text{ T is the number of total Stump}$$

[Example](https://www.youtube.com/watch?v=LsK-xG1cLYA&list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&index=53)

#### Bagging Vs Boosting

- Both have final prediction as a <span style="color:red">linear combination of classifiers</span>
- **Bagging** combination weights are uniform; **boosting** weights (amount of say $$\alpha_t$$) are a measure of performance for classifier at round
-  **Bagging** has **independent** classifiers, **boosting** ones are **dependent** of each other
- <span style="background-color:#FFFF00">**Bagging** randomly selects training sets; **boosting**
focuses on most difficult points</span>

## Gradient Boost

**Pros**:

- Often best-off-shelf accuracy on many problems
- Using modle for prediction requires only modest memory and is fast 
- <span style="background-color:#FFFF00">**Doesn't requre careful normalization of features to perform well**</span>
- Like decision trees, handles a mixture of feature types 

**Cons**:

- Like Random Forest, the model are difficult for humans to interpret
- <span style="background-color:#FFFF00">**Requires careful tuning of the learning rate and other parameters**</span>
- Training can require significant computation
- Like Decision trees, <span style="color:red">not recommended for text classification and other problems with very high dimensional sparse features</span>, for accuracy and computational cost reasons


#### Gradient Boost for Regression

- Gradient Boost starts by making a single **leaf**, instead of a tree or stump. The leaf represents an initial guess for the **Weights** of all of the samples
- **Like Adaboost, Gradient Boost builds fixed sized trees based on the previous tree's errors**
- Like Adaboost, Graident Boost scale the trees, **However, Gradient Boost all the trees by the same amount** 
  - <span style="background-color:#FFFF00">Gradient Boost use a **Learning Rate**(Value between 0 and 1) to scale the contribution from the tree instead of **Amount of Say**</span>
- **Gradient Boost** usually uses trees larger than **stumps**, In practice, people often set the maximum number of leaves to between 8 and 32
- Gradient Boost continues to build trees until it has made the number of trees you asked for or additional trees fail to improve the fit





#### Build Gradient Boost Regression Trees

<hr>

$$

\begin{aligned}

Input: & Data \{ \left( x_i , y_i \right) \}_{i=1}^m, \text{and a differentiable Loss Function} L \left( y, F\left( x \right) \right) \\

\text{Step 1: } & \text{Initialize model with a constant value } \color{Fuchsia}{F_0\left(x \right) = argmin_\gamma \sum_{i=1}^n L \left( y_i, \gamma \right)}\\

\text{Step 2: } & \text{ for } t = \text{1 to T}: \color{Gray}{\text{( T is the number of Trees)}} \\

& \left( A \right) \text{Compute }  \text{ for i = 1...M} \color{Gray}{\text{( M is the number of data)}} \\ 
& \quad \color{Fuchsia}{r_{it} = - \left[ \frac{\partial L \left( y_i, F\left( x_i \right) \right)}{\partial F\left( x_i \right)} \right]_{F\left( x \right) = F_{t-1}\left( x \right)}} \\

& \left( B \right) \text{Fit a regression tree to pseudo-residuals, train it using  } \left( x_i , r_{it} \right) \}_{i=1}^m \\
& \quad \text{and create Terminal regions } R_{jt} \color{grey}{\text{( j-th leaf in the t-th tree)}} \\

& \left( C \right) \text{For j = 1 ... L compute } \color{Fuchsia}{r_{jt} =  argmin_\gamma \sum_{x_i \in R_{jt}} L \left( y_i, F_{t-1}\left( x_i \right) + \gamma \right)}  \\
& \quad \color{grey}{\text{( L is total number of leaves in t-th  tree  )}} \\

& \left( D \right) \text{update the model }, \nu \text{ is learning rate} \color{grey}{\text{ (value between 0 and 1)}}\\
& \quad \color{Turquoise}{F_t\left( x \right) = F_{t-1}\left( x \right) +  \nu \sum_{j}^L r_{jt} I\left( x \in R_{jt} \right)} \\

\end{aligned}
$$

<hr>

**Explanation:**: $$\hat y_i = F_i \left( x \right)$$ : ith tree prediction value


**Input**: Data $$\{ \left( x_i , y_i \right) \}_{i=1}^m$$, and a differentiable Loss Function $$L \left( y_i, F\left( x \right) \right)$$


- $$\{ \left( x_i , y_i \right) \}_{i=1}^m$$ m is the number of training data, and $$x_i$$ is a vector represent input features. $$y_i$$ is the predicted value
- Loss Function $$L \left( y, F\left( x \right) \right) = \frac{1}{2} \sum_{i=1}^m \left( y_i - F\left( x \right)\right)^2 $$, m is total number of data

**Step 1**

$$ F_0\left(x \right) = argmin_\gamma \sum_{i=1}^m L \left( y_i, \gamma \right) $$

<span style="background-color:#FFFF00">means need to find a $\gamma$ value to minimize the sum of Loss function</span>. Can calculate the derivative of Loss function respect to $F_0\left(x \right)$. And set it equal to 0 to find the optimal value
- It is <span style="color:red">**average of y**</span>. It means the prediction just the averge of y of training data. 
- <span style="background-color:#FFFF00">Only $F_0\left(x\right) $ is the same for all $x_i$. Once update the $F_t\left(x_i \right) $ using pseudo-residual, $F_t\left(x_i \right) $  will be different for differernt $x_i$
</span>

$$ \frac{d }{d F_0\left(x \right)}\sum_{i=1}^m L \left( y_i,F_0\left(x \right) \right) = \frac{d }{d F_0\left(x \right)}\sum_{i=1}^m \frac{1}{2}  \left( y_i - F_0\left(x \right) \right)^2 = - \sum_{i=1}^m   \left( y_i - F_0\left(x \right) \right)  $$

$$- \sum_{i=1}^m   \left( y_i - F_0\left(x \right) \right) = 0  \Rightarrow  F_0\left(x \right)= \frac{1}{m}\sum_{i=1}^m   y_i $$

$$F_0\left(x \right)  = \frac{1}{m}\sum_{i=1}^m   y_i $$


T is the number of Trees and usually set to 100 trees

**(A)**: <span style="background-color:#FFFF00">**$$r_{it} $$: t-th tree's  i-th sample Pseudo Residuals**</span>

- for i = 1, ... , m is to calculate residual for all the samples in the dataset

$$r_{it} = - \underbrace{ \left[ \frac{\partial L \left( y_i, F\left( x_i \right) \right)}{\partial F\left( x_i \right)} \right]_{F\left( x \right) = F_{t-1}\left( x \right)}}_{\text{Plug last step } F\left(x \right) \text{  into equation after getting the derivative}} $$

As we know, 

$$- \frac{\partial L \left( y_i, F\left( x_i \right) \right)}{\partial F\left( x_i \right)}  = - \frac{d }{d \hat y}  L \left( y_i, F\left(x_i \right)\right) =  y_i - F\left(x_i \right) $$

When t = 1, 

$$ y_i - F_0\left(x_i \right)   =  y_i - \text{average of y}$$


**(B): <span style="background-color:#FFFF00">means to build a regression tree to predict residuals instead of true y value</span>**

![](/img/post/ML/gb1.png)



**(C)**: if multiple training data end with the same leaf, use this equation to calculate output value. Calulate the derivate of Loss function respect to $$F_{t-1}$$, and set it to 0 to find the optimal value 

- Each time add a tree to the Prediction, the residuals get smaller

$$  \sum_{x_i \in R_{jt}} L \left( y_i, F_{t-1}\left( x_i \right) + \gamma \right) = \sum_{x_i \in R_{jt}} \frac{1}{2} \left( y_i - \left( F_{m-1}\left( x_i \right) + \gamma\right) \right)^2   $$

$$\frac{d}{d \gamma} \sum_{x_i \in R_{jt}} \frac{1}{2} \left( y_i - \left( F_{m-1}\left( x_i \right) + \gamma\right) \right)^2 = $$

$$\begin{aligned} 

\frac{d}{d \gamma} \sum_{x_i \in R_{jt}} \frac{1}{2} \left( y_i - \left( F_{m-1}\left( x_i \right) + \gamma\right) \right)^2 & = -\sum_{x_i \in R_{jt}} \left( y_i -  F_{m-1}\left( x_i \right) - \gamma \right) \\

&= -\sum_{x_i \in R_{jt}} \left( \text{pseudo residual} - \gamma\right)

\end{aligned}$$

$$ \gamma = \frac{1}{\text{number of sample end in } R_{jt}} \sum_{x_i \in R_{jt}}  \text{pseudo residual} 

$$

![](/img/post/ML/gb2.png)



**(D)**: make a new prediction for each sample. Each new prediction is based on last prediction made and the tree just making
  - <span style="background-color:#FFFF00">**A small learning rate reduces effect each tree has on the final prediction and this improves accuracy in the long run**</span>

![](/img/post/ML/gb3.png)
![](/img/post/ML/gb3_1.png)
  
  

#### Gradient Boost for Classification

<span style="color:red">Same algorithm as Regression but cost function changes</span>. Some detailed explanation below.
- $F_0 \left( x \right)$ is the log odds
- <span style="background-color:#FFFF00">**Each leaves prediction is probability** </span>. <span style="background-color:#FFFF00">**Whereas, in Gradient Boost for Regression, a leaf is the average of Residual**</span>
- Each leaves prediction is probability, so need some transformation to log(odds)

<span style="color:red">**Convert Log(odds) to probability**</span>

$$P = \frac{e^{log\left(odd\right)}}{1+ e^{log\left(odd\right)}} = \frac{1}{1+ e^{-log\left(odd\right)}}$$

<span style="color:red">**Convert probability to  Log(odds)**</span>

$$Log\left( odds \right) = \frac{\sum_{\color{red}{leaf}} Residual_i}{\sum_{\color{red}{\text{leaf}}} \text{Previous Probability } \times \left( 1-\text{Previous Probability } \right) }$$

<span style="background-color:#FFFF00">**Evaluation**</span>:

$$Log\left( odds \right) = F_0 \left( x_i \right) + \nu F_1 \left( x_i \right) + \cdots + \nu F_T \left( x_i \right)$$

Then Predict 

$$
y = 
\begin{cases}
1  &  \text{ if }  \frac{1}{1 + e^{Log\left( odds \right)}} > 0.5 \\
0  &   \text{ if } \frac{1}{1 + e^{Log\left( odds \right)}} \leq 0.5 \\
\end{cases}
$$


**Input**: Data $\{ \left( x_i , y_i \right) \}_{i=1}^m$, and a differentiable Loss Function $L \left( y_i, F\left( x \right) \right)$


- $y_i$ is the categorical value
- Loss Function of Binary classification :  $ F\left( x \right)$ refers as predicted probability. 
- The goal is to <span style="color:red">**maximize the loglikehood function and minimize loss function**</span>. So if want to use loglikehood function as cost function,  multiply by -1


$$L \left( y, F\left( x \right) \right) = - \sum_{i=1}^m \left( y_i log \left( \frac{1}{1+ e^{-F\left( x \right)}}  \right) + (1-y_i) log \left( 1-  \frac{1}{1+ e^{-F\left( x \right)}}   \right) \right) $$


**Step 1**:

$$ F_0\left(x \right) = argmin_\gamma \sum_{i=1}^m L \left( y_i, \gamma \right) $$

<span style="background-color:#FFFF00">means need to find a $\gamma$ value to minimize the sum of Loss function</span>. Can calculate the derivative of Loss function respect to $F_0\left(x \right)$. And set it equal to 0 to find the optimal value 
- It is <span style="color:red">**probability of predicted positive class**</span>.

$$\begin{aligned} 

\text{let } a &= \frac{1}{1 + e^{-F_0\left(x\right)}}  \\
\frac{da}{dF_0\left( x \right)} &= \frac{e^{-F_0\left(x\right)}}{\left(1 + e^{-F_0\left(x\right)}\right)^2} \\

& =  \left(1 - \frac{1}{1 + e^{-F_0\left(x\right)}}\right)  \frac{1}{1 + e^{-F_0\left(x\right)}} \\

& = a * \left( 1- a \right)
 \end{aligned}$$


$$\begin{aligned} 

 L\left( y, F_0\left( x \right) \right) &= - \sum_{i=1}^m \big( y_i log \left( a\right) + (1-y_i) log \left( 1-a  \right) \big) \\

\frac{d L \left( y,F_0\left(x \right)\right)}{da} &=\sum_{i=1}^m\left( -\frac{ y_i}{a}+ \frac{ 1- y_i}{ 1- a} \right)\\

\frac{d L \left( y,F_0\left(x \right)\right)}{dF_0\left( x\right)} &= \frac{d L \left( y,F_0\left(x \right)\right)}{da} \frac{da}{dF_0\left( x\right)} \\

&=\sum_{i=1}^m\left( -\frac{ y_i}{a}+ \frac{ 1- y_i}{ 1- a} \right) *a * (1-a) \\

&=\sum_{i=1}^m\left( -y_i(1-a)+ \left(1- y_i\right)a \right) \\

&= \sum_{i=1}^m\left( a - y_i \right) = ma - \sum_{i=1}^m y_i  \\

 \end{aligned}$$


Set the derivative equal to zero and can get 


$$   
\begin{aligned}
\color{fuchsia}{a} & = \color{fuchsia}{ \frac{1}{m} \sum_{i=1}^m  y_i } \\

 &= {\frac{\text{Number of Positive Class}}{\text{Total Number of Sample}}} = \color{blue}{p} \text{ probability of predicted positive} 
\end{aligned}$$


$$F_0\left( x \right) = log\left( \frac{a}{1-a} \right) = \color{blue}{log \left(  \frac{p}{1-p} \right)}$$

<span style="background-color:#FFFF00">**Note: p is only the same for the first tree. It will change when build next tree**</span>

**(A)**: <span style="background-color:#FFFF00">**$$r_{it} $$: t-th tree's  i-th sample Pseudo Residuals**</span>

- for i = 1, ... , m is to calculate residual for all the samples in the dataset

$$r_{it} = - \underbrace{ \left[ \frac{\partial L \left( y_i, F\left( x_i \right) \right)}{\partial F\left( x_i \right)} \right]_{F\left( x \right) = F_{t-1}\left( x \right)}}_{\text{Plug last step } F\left(x \right) \text{  into equation after getting the derivative}} $$

For above,  

$$ 
- \frac{\partial L \left( y_i, F\left( x_i \right) \right)}{\partial F\left( x_i \right)} = a - y_i = \frac{1}{1+e^{-F\left( x \right)} } - y_i\\$$



When t = 1, $ F_0\left( x \right) = log \left(  \frac{p}{1-p} \right)  $

$$ \begin{aligned} 

- \frac{\partial L \left( y_i, F\left( x_i \right) \right)}{\partial F\left( x_i \right)} &= \frac{1}{1+e^{-F\left( x \right)} } - y_i\\

&= \frac{1}{1+e^{-log \left(  \frac{p}{1-p} \right)  } } - y_i \\


&= \frac{1}{1+ \frac{1-p}{p}   } - y_i \\ 

& = p - y_i = \color{fuchsia}{pseudo-residual} \\ 
\end{aligned}$$

**(C)**: Use Talyer Series to approximate the formula, [check here](https://blog.paperspace.com/gradient-boosting-for-classification/)

$$ \text{For j = 1 ... L compute } \color{Fuchsia}{r_{jt} =  argmin_\gamma \sum_{x_i \in R_{jt}} L \left( y_i, F_{t-1}\left( x_i \right) + \gamma \right)}  \\
 \quad \color{grey}{\text{( L is total number of leaves in t-th  tree  )}} $$

$$
\sum_{x_i \in R_{jt}} L\left(y_i, F_{t-1}\left(x_i\right) + \gamma \right) \approx \sum_{x_i \in R_{jt}}L\left(y_i, F_{t-1}\left(x_i\right) \right) + \sum_{x_i \in R_{jt}}\frac{d}{d F_{t-1}\left(x_i \right)}L\left(y_i, F_{t-1}\left(x_i\right) \right)\gamma \\ + \sum_{x_i \in R_{jt}}\frac{1}{2}\frac{d^2}{d F_{t-1}\left(x_i \right)^2}L\left(y_i, F_{t-1}\left(x_i\right) \right)\gamma^2
$$

As we know,


$$
\begin{aligned}
\text{let } p &= \frac{1}{1 + e^{-F_t\left(x\right)}}  \\

\frac{da}{dF_t\left( x \right)} &= p * \left( 1- p \right) \\

\frac{d L \left( y,F_t\left(x \right)\right)}{dF_t\left( x\right)} &= \ \sum_{i=1}^m\left( p - y_i \right) = -\sum_{i=1}^m \text{pseudo-residual} \\

& \quad \color{grey}{\text{ p is the probability of predict positive}}\\

\frac{d^2 L \left( y,F_t\left(x \right)\right)}{dF_t\left( x\right)^2} &= \frac{d}{dF_t\left( x\right)} \sum_{i=1}^m\left( p - y_i \right) = \frac{d}{dF_t\left( x\right)} \sum_{i=1}^m\left( \frac{1}{1+e^{-F_t\left( x\right)}} - y_i \right) \\

& = \sum_{i=1}^m\left( p*(1-p) \right)

\end{aligned}
$$


Take the derivative to $gamma$ on both side


$$\begin{aligned}
\sum_{x_i \in R_{jt}}\frac{d}{d\gamma}L\left(y_i, F_{t-1}\left(x_i\right) + \gamma \right) &\approx \sum_{x_i \in R_{jt}} \frac{d}{d\gamma} L\left(y_i, F_{t-1}\left(x_i\right) \right) + \sum_{x_i \in R_{jt}}\frac{d}{d\gamma} \frac{d}{d F_{t-1}\left(x_i \right)}L\left(y_i, F_{t-1}\left(x_i\right) \right)\gamma  \\

& \quad \quad +\sum_{x_i \in R_{jt}} \frac{1}{2}\frac{d}{d\gamma} \frac{d^2}{d F_{t-1}\left(x_i \right)^2}L\left(y_i, F_{t-1}\left(x_i\right) \right)\gamma^2 \\

&= 0 + \sum_{x_i \in R_{jt}}\frac{d}{d F_{t-1}\left(x_i \right)}L\left(y_i, F_{t-1}\left(x_i\right) \right)+  \sum_{x_i \in R_{jt}}\frac{d^2}{d F_{t-1}\left(x_i \right)^2}L\left(y_i, F_{t-1}\left(x_i\right) \right)\gamma \\

&= - \sum_{x_i \in R_{jt}} \text{pesudo-residual} + \sum_{x_i \in R_{jt}} \left( p * （1-p) \right) \gamma
\end{aligned}$$

And set the derivative to Zero

$$\gamma = \frac{\sum_{x_i \in R_{jt}} \text{pesudo-residual} }{\sum_{x_i \in R_{jt}} \left( p * （1-p) \right) }  = \frac{\sum_{\color{red}{leaf}} \text{residual} }{\sum_{\color{red}{leaf}} \left( p * （1-p) \right) }$$

E.g. use people's income and age to predict if the person has car

![](/img/post/ML/gb4.png)
![](/img/post/ML/gb5.png)
![](/img/post/ML/gb6.png)

<!---video src="https://www.youtube.com/embed/J4Wdy0Wc_xQ" controls></video-->


## XGBoost

- Note: <span style="background-color:#FFFF00">**XGBoost is designed for large, complicated data sets**</span>
- The default of level of trees is 6
- <span style="color:red">**Regardless of classification or regression, default probability is 0.5**</span>at the first step
- <span style="background-color:#FFFF00">**$\lambda$ is regularization parameter. it is intended to reduce the prediction's sensitivity to individual observation**</span>
  - if $lambda > 0$, **similarity scores** are smaller. The amount of decrease is <span style="color:red">**inversely proportional**</span> to the number of <span style="color:red">**Residuals**</span> in the node
    - e.g. only 1 residual in the leaf, $\lambda = 1$, reduce similarity by 50%
    - e.g. only 4 residual in the leaf, $\lambda = 1$, reduce similarity by 20%
  - <span style="background-color:#FFFF00">if $\lambda > 0$, it results in more pruning, by shrinking the **Similarity Scores**, smaller **Output Values** for leave, and **Gain** are smaller</span>
  - <span style="background-color:#FFFF00">set $\lambda = 0$, doesn't prevent tree from pruning</span>, because **$Gain < 0$**, $Gain - \gamma  = Gain $ still negative number, so prune the branch and make it as leaf 
  - setting $\lambda = 1$, prevent overfitting the Training Data
- If **Gain** is less than $\gamma$, remove the branch. Done the pruning. Otherwise keep the branch and continue to prune
- XGBoost learning rate $\eta$ default value is 0.3


$$\text{Leaf Output Value} = \frac{\color{fuchsia}{\text{Sum of Residuals}}}{\text{Number of Residuals } + \lambda } $$

$$\text{Similarity Score} = \frac{\color{fuchsia}{\text{Squared of Sum of Residuals }}}{\text{Number of Residuals } + \lambda } $$

#### XGBoost for Regression

<span style="background-color:#FFFF00">Calculate the **Similarity Score**:</span>

$$\text{Similarity Score} = \frac{\color{fuchsia}{ \left(\sum Residual_i \right)^2 }}{\text{Number of Residuals } + \color{red}{\lambda} } $$

<span style="background-color:#FFFF00">and the **Gain**:</span>

$$Gain = Left_{Similarity} +  Right_{Similarity} -  \underbrace{Root_{Similarity}}_{\text{Similarity Score after prune }}$$

<span style="background-color:#FFFF00">Prune the tree by calculating the differences betwen **Gain** values and predefined **Tree Complexity Parameter $\gamma$**</span>


$$Gain - \gamma = \begin{cases} 
> 0 & \text{do not prune} \\
< 0 & \text{prune} \\
\end{cases}$$

**Then calculate the Output Values for the remaining leaves**

$$\text{Leaf Output Value} = \frac{\color{fuchsia}{\text{Sum of Residuals}}}{\text{Number of Residuals } + \color{red}{\lambda }} $$

#### XGBoost for Clasification

<span style="background-color:#FFFF00">Calculate the **Similarity Score**:</span>


$$\text{Similarity Score} = \frac{\color{fuchsia}{\left( \sum Residual_i \right)^2}}{ \sum \left[ \text{Previous Probability}_i \times \left(1 - \text{Previous Probability}_i \right) \right] + \color{red}{\lambda} } $$




**References**

[StatQuest](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw)