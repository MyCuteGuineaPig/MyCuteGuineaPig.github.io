---
layout:     post
title:      "NLP"
subtitle:   "coursera note"
date:       2020-11-12 20:00:00
author:     "Becks"
header-img: "img/post-bg2.jpg"
catalog:    true
tags:
    - Deep Learning
    - Machine Learning
    - 学习笔记
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

## Natural Language Processing with Classification and Vector Spaces

#### Week1 Vocabulary & Feature Extraction

**Sparse Representation**: 

- Represent Text as a vector. Check every word from vocabulary, if appear, note 1 else 0
- Disadvantage: Large
- Logistric regression learn **N+1** where N is the size of vocabulary
- Build vocabulary: process one by one and get sets of lists of word


```
e.g. I am happy because I am learning NLP

[I, am, happy, because, learning NLP, ... hated, the, movie]
 1   1   1       1         1       1        0     0     0

```



![](/img/post/Natural-Language-Processing/course1/week1pic1.png)

**Positive & Negative Frequencies**: Build vocabulary, map word to frequencey

![](/img/post/Natural-Language-Processing/course1/week1pic2.png)

![](/img/post/Natural-Language-Processing/course1/week1pic3.png)

![](/img/post/Natural-Language-Processing/course1/week1pic4.png)

![](/img/post/Natural-Language-Processing/course1/week1pic5.png)


<span style="background-color: #FFFF00">Faster speed for logistic regression, Instead learning V feature(size of vocabulary), only learn 3 features</span>


![](/img/post/Natural-Language-Processing/course1/week1pic6.png)

![](/img/post/Natural-Language-Processing/course1/week1pic7.png)

![](/img/post/Natural-Language-Processing/course1/week1pic8.png)


Hence you end up getting the following feature vector ```[1,8,11][1,8,11]```. 11 corresponds to the bias, 88 the positive feature, and 1111 the negative feature.


**Preprocessing**: 

When preprocessing, you have to perform the following:

1. Eliminate handles(@...) and URLs
2. Tokenize the string into words.
3. Remove stop words like  “and, is, are, at, has, for ,a”
4. Stemming- or convert every word to its stem. Like dancer, dancing, danced, becomes 'danc'. You can use porter stemmer to take care of this.
5. Convert all your words to lower case.


![](/img/post/Natural-Language-Processing/course1/week1pic9.png)


![](/img/post/Natural-Language-Processing/course1/week1pic10.png)

matrix with m rows and three columns. Every row: the feature for each tweets

![](/img/post/Natural-Language-Processing/course1/week1pic11.png)


**General Implementation**: 

```python
freqs = build_freqs(tweets,labels) #Build frequencies dictionary
X = np.zeros((m,3)) #Initialize matrix X
for i in range(m): #For every tweet
    p_tweet = process_tweet(tweets[i]) #process tweet, include
                                      # deleting stops, stemming, deleting URLS, and handles and lower cases
    X[i, :] = extract_features(p_tweet, freqs) #Extract Features: 
                                               #By summing up the positive and negative frequencies of the tweets
                                               
```

**Logistic Regression**

$$x^{\left( i \right)} $$ is i-th tweet

![](/img/post/Natural-Language-Processing/course1/week1pic12.png)

![](/img/post/Natural-Language-Processing/course1/week1pic13.png)


**Logistic Cost Function Derivation**

$$P\left( y \mid x^{\left( i \right)}, \theta \right) = h \left(x^{\left( i \right)}, \theta \right)^{y^\left( i \right)} \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)^{\left( 1- y^\left( i \right) \right)} $$

when y = 1, you get $$h \left(x^{\left( i \right)}, \theta \right)$$, when y = 0, you get $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right) $$. In either case, you want to maximize $$ h \left(x^{\left( i \right)}, \theta \right)$$ clsoe to 1. when y = 0, you want $$ \left( 1-  h \left(x^{\left( i \right)}, \theta \right) \right)  $$ to be 0  as $$ h \left(x^{\left( i \right)}, \theta \right)$$ close to 1, when y = 1, you want $$ h \left(x^{\left( i \right)}, \theta \right) = 1$$

Define the likelihood as 

$$L\left(  \theta \right) = \prod_{i=1}^{m}  h \left(\theta, x^{\left( i \right)} \right)^{y^\left( i \right)} \left( 1-  h \left(\theta, x^{\left( i \right)} \right) \right)^{\left( 1- y^\left( i \right) \right)}   $$


![](/img/post/Natural-Language-Processing/course1/week1pic14.png)


**Gradient Descent Derivation**

![](/img/post/Natural-Language-Processing/course1/week1pic15.png)

![](/img/post/Natural-Language-Processing/course1/week1pic16.png)

![](/img/post/Natural-Language-Processing/course1/week1pic17.png)


#### Week2 Naive Bayes

Tweet <span style="color:red">defined either positive or negative, cannot be both</span>. Bayes' rule is based on the mathmatical formulation of conditional probabilities. <span style="background-color: #FFFF00">Called **Naive** because make assumpton that the **features you're using for classification are all independent**, which in reality is rare</span>

$$ P\left(X \mid Y \right) = \frac{ P\left(Y \mid X \right) P\left(X \right)}{P\left(Y \right)} $$

e.g. Happy sometimes as positive word sometimes as negative, A the number of positive tweet, B the number tweet contain happy

![](/img/post/Natural-Language-Processing/course1/week2pic1.png)

$$ P\left(Positive \mid happy \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(happy \right)} $$

$$ P\left(happy \mid Positive \right) = \frac{ P\left(Positive \cap happy \right)}{P\left(Positive \right)} $$

$$ P\left(Positive \mid happy \right) =  P\left(happy \mid Positive \right) \frac{ P\left(Positive \right)}{P\left(happy \right)} $$

Q: Suppose that in your dataset, 25% of the positive tweets contain the word ‘happy’. You also know that a total of 13% of the tweets in your dataset contain the word 'happy', and that 40% of the total number of tweets are positive. You observe the tweet: ''happy to learn NLP'. What is the probability that this tweet is positive? <br/>
A: 0.77

![](/img/post/Natural-Language-Processing/course1/week2pic2.png)

- Word with equal probability for positive/negative don't add anything to the sentiment (e.g. I, am, learning, NLP) 
- "sad, happy, not" have a significant difference between probabilities. 
- "because" negative class is zero, no way of comparing between two corpora, a problem

![](/img/post/Natural-Language-Processing/course1/week2pic3.png)


Once you have the probabilities, you can compute the likelihood score as follows. A score greater than 1 indicates that the class is positive, otherwise it is negative.


![](/img/post/Natural-Language-Processing/course1/week2pic4.png)


**Laplacian Smoothing**

We usually compute the probability of a word given a class as follows:

$$ P\left(w_i \mid class \right) = \frac{freq\left(wi,class \right)}{N_{class}} \quad \text{class} \in \text{\{Positive, Negative\}}$$

- $$freq_{positive}$$ and $$freq_{negative}$$  are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.
- $$N_{positive}$$ and $$N_{negative}$$  are the total number of positive and negative words for all documents (for all tweets), respectively.


However, if a word does not appear in the training, then it automatically gets a probability of 0, to fix this we add smoothing as follows

$$ P\left(w_i \mid positive \right) = \frac{freq\left(wi,positive \right) + 1}{N_{positive} + V}$$

$$ P\left(w_i \mid negative \right) = \frac{freq\left(wi,negative \right) + 1}{N_{negative} + V}$$

Note that we added all in the numerator, and since there are V words to normalize, we add V in the denominator. where 


<span style="color:red">$$V$$:  the number of unique words in the entire set of documents, for all classes, whether positive or negative.</span>


![](/img/post/Natural-Language-Processing/course1/week2pic5.png)

**Log Likelihood**

![](/img/post/Natural-Language-Processing/course1/week2pic6.png)


To compute the log likelihood, we need to get the ratios and use them to compute a score that will allow us to decide whether a tweet is positive or negative.  <span style="background-color: #FFFF00">The higher the ratio, the more positive the word is</span>. To do inference, you can compute the following:



$$ \frac{P\left(pos\right)}{P\left(neg\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} > 1 $$



m gets larger, we can get numerical flow issues, hard to store on device , so we introduce the log, which gives you the following equation 

$$ log\left( \frac{P\left( D_{pos}\right)}{P\left(D_{neg}\right)} \prod_{i=1}^m \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)} \right) =>  \underbrace{log\frac{P\left(D_{pos}\right)}{P\left(D_{neg}\right)}}_{\text{log prior}} + \underbrace{\sum_{i=1}^m log \frac{P\left(w_i \mid pos\right)}{P\left(w_i \mid neg\right)}}_{\text{log likelihood}} $$

$$\text{where} \quad P\left(D_{pos}\right) = \frac{D_{pos}}{D},  P\left(D_{neg}\right) = \frac{D_{neg}}{D}$$
 
 <span style="background-color: #FFFF00">where D is the total number of documents, $$D_{pos}$$ is the total number of positive tweets and $$D_{neg}$$ is the total number of negative tweets.</span> <span style="color:red">注：与$$N_{pos}$$ 不同， $$N_{pos}$$ 是total number of all positive words</span>
 
<span style="background-color: #FFFF00">log prior represents the underlying probability in the target population that a tweet is positive versus negative.  important for unbalanced dataset</span>. We further introduce λ as follows:

![](/img/post/Natural-Language-Processing/course1/week2pic7.png)

<span style="color:red">The positive value indicates tweet is positive. Negative value indicates tweet is negative</span>

![](/img/post/Natural-Language-Processing/course1/week2pic8.png)

**Training Naive Bayes**

1. collect and annotate corpus: divide tweets into two groups, positive/negative
2. Preprocess the tweets: process_tweet(tweet) ➞ [w1, w2, w3, ...]:
   - Lowercase
   - Remove punctuation, urls, names
   - Remove stop words
   - Stemming
   - Tokenize sentences
3. Compute freq(w, class), use below table to compute
4. Get $$P\left(w \mid  pos \right), P\left(w \mid neg \right)$$ using Laplacian Smoothing formula
5. Get $$\lambda\left( w \right) =  log \frac{P\left( w \mid pos \right)}{P\left( w \mid neg \right)}$$
6. Compute $$logprior = log \frac{D_{pos}}{D_{neg}} $$ where $$ D_{pos} $$ and $$D_{neg}$$ correspond to  the number of positive and negative documents respectively


![](/img/post/Natural-Language-Processing/course1/week2pic9.png)


**Testing Naive Bayes**

if a word not in vocabulary, think as neutral, not contribute to score

![](/img/post/Natural-Language-Processing/course1/week2pic10.png)

$$\text{Accuracy} \rightarrow \frac{1}{m} \sum_{i=1}^{m} {pred_i == Y_{val}} $$


**Naive Bayes Application**

There are many applications of naive Bayes including:

- Author identification: document written by which author, calculate $$\lambda$$ for each word for both author to build vocabulary
- Spam filtering: $$\frac{P\left(spam \mid email \right)}{P\left(nnspam \mid email \right)}$$
- Information retrieval: filter relevant and irrelevant documents in a database, calculate the likelihood of the documents given the query, retrieve document if $$P\left(document_k \mid query \right) \approx \prod_{i=0}^{\mid query \mid} P\left(query_i \mid document_k  \right) > threshold$$
- Word disambiguation: e.g. don't know bank in reading, refer to river or money? to calculate the score of the document, $$\frac{P\left(river \mid text\right)}{P\left(money \mid text\right)}$$. if refer to river not money, score is bigger than 1. 


**Naive Bayes Assummtion**

<span style="background-color: #FFFF00">Advantage: simple doesn't require setting any custom parameters</span>.

Assumption:

1. <span style="background-color: #FFFF00">Indepdence between predictors or features associated with each class</span>
  - assume word in a piece of text are independent. e.g. "It is sunny and hot in the Sahara desert." But "sunny" and "hot" often appear together. And together often related to beach or desert. Not always independent 
  - <span style="color:red">Under or over estimate conditional probabilities of individual words</span>
  - e.g. "It's always cold and snowy in ___" naive model will assign equal weight to the words "spring, summer, fall, winter".
2. Relative frequencies in corpus: <span style="background-color: #FFFF00">rely on distribution on training dataset</span>
   - A good dataset contain the same proportion of positive and negative weets as a random sample. But in reality positive tweet is more common(because negative tweets maybe muted or banned by twitter). This would result a very optimistic or pessimistic model


**Error Analysis**

1. Semantic meaning lost in the pre-processing step by removing punctuation and stop words
   - "My beloved grandmother :(", with punctuation indicating a sad face, after process, [belov, grandmoth], positive tweet
   - "This is not good, because your attitude is not even close to being nice." if remove neutral word, [good, attitude, close, nice] positive
2. Word order affect meaning
   - "I am happy because I did not go." vs. "I am not happy because I did go."
3. Adversarial attack: some quirks of languages come naturally to human but confuse Naive Bayes model
   - some common language phenomenon, Sarcasm, Irony and Euphemisms.
   - e.g. "This is a ridiculously powerful movie. The plot was gripping and I cried right through until the ending" positive. After process. [ridicul, power, movi, plot, grip, cry, end] negative



#### Week3 Vector Space

e.g. "Where are you heading" vs "Where are you from" -> Different meaning 尽管前三个词一样<br/>
e.g. "What is your age" vs "How old are you" -> Same meaning

e.g. "You eat ceral from  a bowl" ceral and bowl are related <br/>
e.g. "You can buy something and someone else sells it" second half sentence is dependent on the first half

- Vector space: <span style="background-color: #FFFF00">identify the context around each word  to capture relative meaning, to represent words and documents as vector</span>
- Vector space allow to identify sentence are similar even if they do not share the same words
- Vector space allow to capture dependencies between words
- application:
  -  information extraction to answer questions in the style of who, what, where, how... 
  -  machine translation
  -  chatbots programming: 机器客服


**Word by Word Design**

number of times they <span style="background-color: #FFFF00">occur together within a certain **distance** k</span>


 <span style="background-color: #FFFF00">Co-occurence matrix</span>: e.g. data =2, count how many appearance for  each word has distance smaller or euqal than 2


![](/img/post/Natural-Language-Processing/course1/week3pic1.png)

<span style="color:red">Word by word , get n entries, with n between 1 and size(vocabulary)</span>



**Word by Document Design**

number of times they <span style="background-color: #FFFF00">occur together within a certain **category** k</span>

Divide corpus into different tops like Entertainment, Economy, Machine Learning. Below data appear 500 times in corpus related to Entertainment, 6620 times in documents related to Economy

![](/img/post/Natural-Language-Processing/course1/week3pic2.png)

![](/img/post/Natural-Language-Processing/course1/week3pic3.png)

**Euclidean Distance**

Euclidean Distance is the norm of the difference between vectors, straight line between points

$$d\left( \vec{v}, \vec{w} \right) = \sqrt{\sum_{i=1}^n \left(v_i - w_i \right)^2 } \quad \rightarrow \quad \text{Norm of }  \left( \vec{v}, \vec{w} \right) $$

![](/img/post/Natural-Language-Processing/course1/week3pic4.png)

```python
v = np.array([1,6,8])
w = np.array([0,4,6])

#Calculate the Eudlidean distance d 

d = np.linalg.norm(v-w)
```


**Cosine Similarity**

Problem with Euclidean distance:  Vector space where corpora are represented by occurrence of the words "disease" and "eggs". Because Agriculture and history corpus has a similar number of words(出现单词数量一样), Suggests agriculture and history 比 agriculture and food 更想近义

- if A and B are identical, you will get $$cos\left( \theta \right)=1$$
- If you get $$cos\left( \theta \right)=0$$, that means that they are orthogonal (or perpendicular).
- Numbers between 0 and 1 indicate a similarity score.
- Numbers between -1-0 indicate a dissimilarity score.
- <span style="background-color: #FFFF00">Advantage: Cosine similarity isn't biased by the size difference between the representations</span>

![](/img/post/Natural-Language-Processing/course1/week3pic5.png)

$$\begin{align} \hat{v} \cdot \hat{w} &= \Vert \hat{v} \Vert \Vert \hat{w} \Vert cos \left( \beta \right) \\  cos \left( \beta \right) &= \frac{\hat{v} \cdot \hat{w}}{\Vert \hat{v} \Vert \Vert \hat{w} \Vert}  \\ &= \frac{\left(20 \times 30\right) + \left(40 \times 20 \right)}{\sqrt{20^2 + 40^2} \times \sqrt{30^2 + 20^2}}\end{align}$$

-  proportional to the similarity between the directions of the vectors that you are comparing. 
-  the cosine similarity values between 0 and 1 for word by Word/Docs, cosine similarity values between -1 and 1 when using word embedding 

![](/img/post/Natural-Language-Processing/course1/week3pic6.png)

**PCA**

- PCA: dimenison reduction from high dimension to fewer dimension. project data into a lower dimension while retaining as much information as possible
- Original Space -> Uncorrelated features -> Dimension Reduction
- Can use visualization to check if PCA capure relationship among words
- <span style="color:red">**Eigenvector**</span>: <span style="background-color: #FFFF00">directions</span> of uncorrelated features for your data
- <span style="color:red">**Eigenvalue**</span>: amount of information retained by each feature(<span style="background-color: #FFFF00">variances</span> of data in each of those new features)
- <span style="color:red">**Dot Product**</span>: gives the <span style="background-color: #FFFF00">projection</span> on uncorrelated features

![](/img/post/Natural-Language-Processing/course1/week3pic7.png)

1. Mean Normalize Data  $$x_i = \frac{x_i - \mu_i}{\sigma_{x_{i}}}$$
2. Get Covariance Matrix
3. Perform Singular Value Decomposition to get set of three matrices(Eigenvectors on column wise, Eigenvalues on diagonal). <span style="background-color: #FFFF00"> Eigenvalues and eigenvectors should be organized according to the eigenvalues in descending order. Ensure to retain as much as information as possible </span>
4. Project data to a new set of features, using the eigenvectors and eigenvalues in this steps


![](/img/post/Natural-Language-Processing/course1/week3pic8.png)

![](/img/post/Natural-Language-Processing/course1/week3pic9.png)



#### Week4 Transforming word vectors

Translate word: caculate word embedding in English and French, transform word embedding in English to French and find the most similar one

![](/img/post/Natural-Language-Processing/course1/week4pic1.png)

R: transform matrix, X: English word, Y: French word.  First get subsets of English word and French equivalence

![](/img/post/Natural-Language-Processing/course1/week4pic2.png)

<span style="color:red">Why not store all vocabulary?</span>: just need a subset to collect to find transform matrix. If it works well, then model can translate words that are not in vocabulary

**Solving for R**:

$$ \begin{align} \color{olive}{\text{Initialize R}}\quad & \\
 \color{blue}{\text{in a loop:}}\quad & \\ 
 & Loss = \Vert XR - Y \Vert_F  \qquad   \color{fuchsia}{\text{Frobenium Norm}} \\
 & g = \frac{d}{dR} Loss \qquad   \color{fuchsia}{\text{gradient R}}  \\
 & R = R - \alpha g \qquad \quad   \color{fuchsia}{\text{update}} 
\end{align} $$ 


Can pick a fixed number of times to go through the loop or check the loss at each iteration and break out loop when loss falls between a certain threshold.


<span style="background-color: #FFFF00">Explanation for fixed number of iterations iterating until the loss falls below a threshold.</span>

- You cannot rely on training loss getting low -- what you really want is the validation loss to go down, or validation accuracy to go up. And indeed - in some cases people train until validation accuracy reaches a threshold, or -- commonly known as "early stopping" -- until the validation accuracy starts to go down, which is a sign of over-fitting.
- Why not always do "early stopping"? Well, mostly because well-regularized models on larger data-sets never stop improving. Especially in NLP, you can often continue training for months and the model will continue getting slightly and slightly better. This is also the reason why it's hard to just stop at a threshold -- unless there's an external customer setting the threshold, why stop, where do you put the threshold?
- Stopping after a certain number of steps has the advantage that you know how long your training will take - so you can keep some sanity and not train for months. You can then try to get the best performance within this time budget. Another advantage is that you can fix your learning rate schedule -- e.g., lower the learning rate at 10% before finish, and then again more at 1% before finishing. Such learning rate schedules help a lot, but are harder to do if you don't know how long you're training.

**Frobenius norm**


$$ \Vert A \Vert_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n \vert a_{ij}\vert^2 } $$ 

e.g. has two word embedding and dimension is two, the matrix is 2x2

$$ A = \begin{pmatrix}
    2 & 2\\
    2 & 2\\
  \end{pmatrix}$$

$$  \Vert A \Vert_F = \sqrt{2^2 + 2^2 + 2^2 + 2^2} = 4 $$

```python
A = np.array([[2,2,],
              [2,2,]])
A_squared = np.square(A)
print(A_squared)  # array([[4,4,],

                  #        [4,4,]])

A_Frobenious = np.sqrt(np.sum(A_squared))
```

**Frobenius norm squared**

$$\frac{1}{m} \Vert XR - Y \Vert_F^2 $$

- The norm is always nonnegative (we're summing up absolute values), and so is the square.
- When we take the square of all non-negative (positive or zero) numbers, the order of the data is preserved. For example, if 3 > 2, 3^2 > 2^2
- Using the norm or squared norm in gradient descent <span style="color:red">results in the same location of the minimum</span>.
- Squaring cancels the square root in the Frobenius norm formula. Because of the chain rule, we would have to do more calculations if we had a square root in our expression for summation.
- Dividing the function value by the positive number doesn't change the optimum of the function, for the same reason as described above.
- divide by m: We're interested in transforming English embedding into the French. Thus, it is more important to measure average loss per embedding than the loss for the entire dictionary (which increases as the number of words in the dictionary increases).



so the gradient will be: 

**Gradient**

Easier to take derivative rather than dealing with the square root in Frobenius norm

$$  Loss = \Vert XR - Y \Vert_F^2  $$

$$  g = \frac{d}{dR}Loss = \frac{2}{m} X^T \left( XR - Y  \right) $$



**Locality Sensitive Hashing**

<span style="background-color: #FFFF00">Reduce computational cost(much faster) to find K-nearest neighbor </span>is to use locality sensitive hashing. For example, using word vectors with just two dimensions. **plan** can helps bucket vectors into subsets based on localtion(蓝色点在蓝色plane 上方，灰色点在灰色plane上方)

![](/img/post/Natural-Language-Processing/course1/week4pic3.png)

normal vector: perpendicular to any vectors on the plane.

![](/img/post/Natural-Language-Processing/course1/week4pic4.png)

If dot product positive, it's on one side of the plane. If dot product negative, it's on opposite side of the plane. If dot product is zero, it is on plane

dot product of P and V1 is equal to the do length of P multiply V1 projection onto P

![](/img/post/Natural-Language-Processing/course1/week4pic5.png)

![](/img/post/Natural-Language-Processing/course1/week4pic6.png)

```python
def side_of_plane(P,v):
  dotprodcut = np.dot(P,v.T)
  sign_of_dot_product = np.sign(dotproduct)
  sign_of_dot_prodcut_scalar = np.asscalar(sign_of_dot_product)
  return sign_of_dot_prodcut_scalar
```

**Multiple Plane**

To divide vector space into manageable regions, want to use more than one plane. Get multiple signal, one for each plane, but need a single hash value 


![](/img/post/Natural-Language-Processing/course1/week4pic7.png)

Rule: 

$$  \color{maroon}{\text{sign_i} \geq 0, \rightarrow h_i = 1}  $$

$$  \color{maroon}{\text{sign_i} < 0, \rightarrow h_i = 0 } $$

$$  \mathbf{hash = \sum_i^H 2^i \times h_i} $$

```python
def hash_multiple_plane(P_l, v):
    hash_value = 0
    for i, P in enumerate(P_l):
        sign = side_of_plane(P,v)
        hash_i = 1 if sign >=0 else 0
        hash_value += 2**i * hash_i
    return hash_value
```



```python
#Generate Random Plane

num_dimensions = 2 
num_planes = 3

random_planes_matrix = np.random.normal(
                       size = (num_planes,
                               num_dimensions)) 
#find out whether vector v is on positive or negative side of each planes

def side_of_plane_matrix(P,v):
  dotproduct = np.dot(P,v.T
  sign_of_dot_product = np.sign(dotproduct)
  return sign_of_dot_product

v = np.array([[2,2,]])
num_planes_matrix = side_of_plane_matrix(random_planes_matrix, v)
```


Text can be embedded into vector space so that nearest neighbors refer to text with similar meaning

![](/img/post/Natural-Language-Processing/course1/week4pic8.png)



## Natural Language Processing with Probabilistic Models

#### Week1 Autocorrect and Minimum Edit Distance


How it works?

1. Identify a misspelled word(only spelling error, not contextual error(e.g. Happy birthday to deer friend))   - how ? if spelled correctly, it is in dictionary. if not, probably a misspelled word
2. Find strings n edit distance way  e.g. deah vs dear
   - Edit: an operation perfomed on a string to change it, insert, delete, switch (eta - > eat, eta -> tea, but not eta -> ate), replace
   - Autocorrect: n is usually 1 to 3 edits
3. Filter candidates 
   - step 2 generate word if not in vocabulary, remove it. e.g. deah -> d_ar, if d_ar not in vocabulary remove it
4. Calculate word probabilities

![](/img/post/Natural-Language-Processing/course2/week1pic1.png)

Minimum distance application: spelling correction, document similarity, machine translation, DNA sequencing etc

Minimum edit distance(**Levenshtein distance**): insert(cost 1), delete(cost 1), <span style="background-color: #FFFF00">replace(cost 2, can think of delete then insert)</span>

![](/img/post/Natural-Language-Processing/course2/week1pic2.png)
